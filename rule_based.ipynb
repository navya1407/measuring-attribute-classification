{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rs3NAroIh-ki",
        "outputId": "231eb8f7-6fdd-4653-8e09-1c67ddbd478b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NoGUmdQEiEgI"
      },
      "outputs": [],
      "source": [
        "#reading the processed sentences and output we got from bonie\n",
        "d={}\n",
        "\n",
        "with open('/content/drive/MyDrive/MTP Project/Sentences/dimension.txt','r') as f:\n",
        "       data = f.read()\n",
        "\n",
        "sentences_output=data.split(\"\\n\\n\")\n",
        "for sent in sentences_output:\n",
        "    l=sent.split('\\n')\n",
        "    d[l[0]]=l[1:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYdMctsViMDI",
        "outputId": "f50e9cc6-7158-4207-fe11-1d9a412b28ee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1280991"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "data=None\n",
        "len(d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "megNy2V_iPlt"
      },
      "outputs": [],
      "source": [
        "\n",
        "#splitted dictionary into 3 parts and ran for each part seperately due to memory error\n",
        "n = len(d) // 3\n",
        "#dict_part1 = dict(list(d.items())[:n])\n",
        "dict_part2 = dict(list(d.items())[n:2*n])\n",
        "#dict_part3 = dict(list(d.items())[2*n:])\n",
        "#len(dict_part3),len(dict_part2),len(dict_part1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YPT7ppeXigwY"
      },
      "outputs": [],
      "source": [
        "#considering only those outputs from bonie which have confidence score greater than or equal t0 0.9\n",
        "new_d={}\n",
        "for key,value in dict_part2.items():\n",
        "\n",
        "    if(key):\n",
        "\n",
        "         values=[]\n",
        "         for result in value:\n",
        "                if(result):\n",
        "                      try:\n",
        "                           score=float(result.split()[0])\n",
        "                           if(score>=0.9):\n",
        "                                  values.append(result)\n",
        "                      except:\n",
        "                             continue\n",
        "         if(values):\n",
        "\n",
        "                  new_d[key]=values\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDdY_uVti0fw",
        "outputId": "264e8fc1-363b-46a2-ce2f-ecc5c1277599"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "335304"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "len(new_d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Vllti5aZi6vM"
      },
      "outputs": [],
      "source": [
        "d=dict_part2=None #just freeing up memory"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_d"
      ],
      "metadata": {
        "id": "phYeV_cad5SY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NAwWYpJYjDyp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04fbcfa3-4009-48d4-d546-5e13e18f7aca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 1.22.4\n",
            "Uninstalling numpy-1.22.4:\n",
            "  Would remove:\n",
            "    /usr/local/bin/f2py\n",
            "    /usr/local/bin/f2py3\n",
            "    /usr/local/bin/f2py3.10\n",
            "    /usr/local/lib/python3.10/dist-packages/numpy-1.22.4.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/numpy.libs/libgfortran-040039e1.so.5.0.0\n",
            "    /usr/local/lib/python3.10/dist-packages/numpy.libs/libopenblas64_p-r0-2f7c42d4.3.18.so\n",
            "    /usr/local/lib/python3.10/dist-packages/numpy.libs/libquadmath-96973f99.so.0.0.0\n",
            "    /usr/local/lib/python3.10/dist-packages/numpy/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled numpy-1.22.4\n",
            "Found existing installation: scipy 1.10.1\n",
            "Uninstalling scipy-1.10.1:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.10/dist-packages/scipy-1.10.1.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/scipy.libs/libgfortran-040039e1.so.5.0.0\n",
            "    /usr/local/lib/python3.10/dist-packages/scipy.libs/libopenblasp-r0-41284840.3.18.so\n",
            "    /usr/local/lib/python3.10/dist-packages/scipy.libs/libquadmath-96973f99.so.0.0.0\n",
            "    /usr/local/lib/python3.10/dist-packages/scipy/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled scipy-1.10.1\n",
            "Found existing installation: scikit-learn 1.2.2\n",
            "Uninstalling scikit-learn-1.2.2:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.10/dist-packages/scikit_learn-1.2.2.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0\n",
            "    /usr/local/lib/python3.10/dist-packages/sklearn/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled scikit-learn-1.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall numpy\n",
        "!pip uninstall scipy\n",
        "!pip uninstall scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "pBJvBSDfjLw9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c27de96-821a-4744-d829-b1135d77ed8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting numpy==1.21\n",
            "  Downloading numpy-1.21.0.zip (10.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: numpy\n",
            "  Building wheel for numpy (pyproject.toml) ... \u001b[?25l\u001b[?25hcanceled\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip3 install numpy==1.21"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITvtVyuujT5e",
        "outputId": "09b6d5ff-64d9-46a7-80d0-d836827f3e81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn==1.2.0 in /usr/local/lib/python3.10/dist-packages (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.0) (1.25.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.0) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.0) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.0) (3.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.25.0)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip3 install scikit-learn==1.2.0\n",
        "!pip3 install scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bjKZqwBjVn2",
        "outputId": "bb86f097-4450-4df2-9cb4-de6f88f3a5a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: quantulum3 in /usr/local/lib/python3.10/dist-packages (0.9.0)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.10/dist-packages (from quantulum3) (6.0.4)\n",
            "Requirement already satisfied: num2words in /usr/local/lib/python3.10/dist-packages (from quantulum3) (0.5.12)\n",
            "Requirement already satisfied: pydantic>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from inflect->quantulum3) (1.10.9)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.10/dist-packages (from num2words->quantulum3) (0.6.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.1->inflect->quantulum3) (4.6.3)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip3 install quantulum3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcyBnal_jeHJ",
        "outputId": "c6a82688-48ab-42af-f432-2364c1e42444"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting stemming\n",
            "  Downloading stemming-1.0.1.zip (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: stemming\n",
            "  Building wheel for stemming (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stemming: filename=stemming-1.0.1-py3-none-any.whl size=11122 sha256=687e18f721f47847f36054710cc11f8667de9605803482db0952fe8fee62c3d3\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/f9/3f/8fe1ec182ce66145d7ded39bad9a690960469dc350fbec0181\n",
            "Successfully built stemming\n",
            "Installing collected packages: stemming\n",
            "Successfully installed stemming-1.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip3 install stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "12sAvDEPjeKQ"
      },
      "outputs": [],
      "source": [
        "#restart run time as i was getting error i uninstalled and installed the specific versions\n",
        "\n",
        "from quantulum3 import parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXXEsQJzjeNB",
        "outputId": "f5497b67-897c-42ad-b1ad-dc32b77d6547"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:288: UserWarning: Trying to unpickle estimator TfidfTransformer from version 1.2.2 when using version 1.2.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:288: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 1.2.2 when using version 1.2.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:288: UserWarning: Trying to unpickle estimator SGDClassifier from version 1.2.2 when using version 1.2.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "WARNING:quantulum3.classifier:The classifier was built using a different scikit-learn version (=1.2.2, !=1.2.0). The disambiguation tool could behave unexpectedly. Consider running classifier.train_classfier()\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Quantity(90, \"Unit(name=\"percentage\", entity=Entity(\"dimensionless\"), uri=Percentage)\"),\n",
              " Quantity(0.5, \"Unit(name=\"dimensionless\", entity=Entity(\"dimensionless\"), uri=Dimensionless_quantity)\"),\n",
              " Quantity(1975, \"Unit(name=\"dimensionless\", entity=Entity(\"dimensionless\"), uri=Dimensionless_quantity)\"),\n",
              " Quantity(2008, \"Unit(name=\"dimensionless\", entity=Entity(\"dimensionless\"), uri=Dimensionless_quantity)\")]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "#example to check whether packages are installed properly or not\n",
        "s=\"More than 90 % of the native rainforest of Paraguay 's eastern half have been lost between 1975 and 2008 .\"\n",
        "parser.parse(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuIo8eEDg7g1",
        "outputId": "59cc75e8-86d6-46ae-fd1f-1d2f09d30e71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting textacy\n",
            "  Downloading textacy-0.13.0-py3-none-any.whl (210 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (5.3.1)\n",
            "Requirement already satisfied: catalogue~=2.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (2.0.8)\n",
            "Collecting cytoolz>=0.10.1 (from textacy)\n",
            "  Downloading cytoolz-0.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting floret~=0.10.0 (from textacy)\n",
            "  Downloading floret-0.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (314 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.7/314.7 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jellyfish>=0.8.0 (from textacy)\n",
            "  Downloading jellyfish-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (1.2.0)\n",
            "Requirement already satisfied: networkx>=2.7 in /usr/local/lib/python3.10/dist-packages (from textacy) (3.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (1.25.0)\n",
            "Collecting pyphen>=0.10.0 (from textacy)\n",
            "  Downloading pyphen-0.14.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (2.27.1)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (1.2.0)\n",
            "Requirement already satisfied: spacy~=3.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (3.5.3)\n",
            "Requirement already satisfied: tqdm>=4.19.6 in /usr/local/lib/python3.10/dist-packages (from textacy) (4.65.0)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from cytoolz>=0.10.1->textacy) (0.12.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->textacy) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->textacy) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->textacy) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->textacy) (3.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0->textacy) (3.1.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (8.1.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (2.4.6)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (6.3.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (1.10.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy~=3.0->textacy) (4.6.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy~=3.0->textacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy~=3.0->textacy) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy~=3.0->textacy) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy~=3.0->textacy) (2.1.3)\n",
            "Installing collected packages: pyphen, jellyfish, floret, cytoolz, textacy\n",
            "Successfully installed cytoolz-0.12.1 floret-0.10.3 jellyfish-1.0.0 pyphen-0.14.0 textacy-0.13.0\n"
          ]
        }
      ],
      "source": [
        "pip install textacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "5lPBwscohPV2"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import textacy\n",
        "from spacy.matcher import DependencyMatcher\n",
        "from spacy.util import filter_spans\n",
        "from spacy.matcher import Matcher\n",
        "import re\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "hk_aeZjdhx5y"
      },
      "outputs": [],
      "source": [
        "'''The merge_phrases function takes a spaCy doc object as input and merges noun phrases (noun chunks) in the document into single tokens.\n",
        " The merged tokens are annotated with attributes such as part-of-speech tag, lemma, and entity type.'''\n",
        "def merge_phrases(doc):\n",
        "    with doc.retokenize() as retokenizer:\n",
        "        for np in list(doc.noun_chunks):\n",
        "            attrs = {\n",
        "                \"tag\": np.root.tag_,\n",
        "                \"lemma\": np.root.lemma_,\n",
        "                \"ent_type\": np.root.ent_type_,\n",
        "            }\n",
        "            retokenizer.merge(np, attrs=attrs)\n",
        "    return doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "7KzjleawhTLa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2012dbee-d04e-4f6b-ac7b-72c4d9e3a3a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The quick brown fox fox NOUN \n",
            "jumps jump VERB \n",
            "over over ADP \n",
            "the lazy dog dog NOUN \n",
            ". . PUNCT \n"
          ]
        }
      ],
      "source": [
        "# Example sentence\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# Process the sentence\n",
        "doc = nlp(sentence)\n",
        "\n",
        "# Apply the merge_phrases function\n",
        "merged_doc = merge_phrases(doc)\n",
        "\n",
        "# Print the merged document\n",
        "for token in merged_doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.ent_type_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "mNbHY9oIiHoe"
      },
      "outputs": [],
      "source": [
        "'''The pattern_1 function applies a dependency-based pattern matching using the DependencyMatcher from spaCy to identify a specific pattern in a sentence.\n",
        "It looks for a sequence of tokens that follow the pattern of a verb, a proper noun as the subject, and a noun as the direct object.\n",
        " The function returns the text of the first token that matches this pattern.\n",
        "\n",
        "Here's a breakdown of the function:\n",
        "\n",
        "The DependencyMatcher is initialized with the spaCy vocabulary (nlp.vocab).\n",
        "\n",
        "The pattern_1 list defines the pattern to be matched. It consists of three dictionaries representing the constraints for three consecutive tokens.\n",
        "\n",
        "The first dictionary specifies that the token on the right should have the part-of-speech tag \"VERB\".\n",
        "\n",
        "The second dictionary specifies that the token on the right should have the part-of-speech tag \"PROPN\" (proper noun) and the dependency label \"nsubj\" (subject) with respect to the previous token.\n",
        "\n",
        "The third dictionary specifies that the token on the right should have the part-of-speech tag \"NOUN\" and the dependency label \"dobj\" (direct object) with respect to the previous verb token.'''\n",
        "\n",
        "def pattern_1(sent):\n",
        "    matcher_1 = DependencyMatcher(nlp.vocab)\n",
        "\n",
        "    pattern_1 = [\n",
        "    {\n",
        "        \"RIGHT_ID\": \"verb\",\n",
        "        \"RIGHT_ATTRS\": {\"POS\": \"VERB\"} #\"ORTH\": {\"IN\":[\"HAS\",\"HAD\",\"HAVE\"]}\n",
        "    },\n",
        "    {\n",
        "        \"LEFT_ID\": \"verb\",\n",
        "        \"REL_OP\": \">\",\n",
        "        \"RIGHT_ID\": \"entity\",\n",
        "\n",
        "        \"RIGHT_ATTRS\": {\"POS\": \"PROPN\",\"DEP\": \"nsubj\"},\n",
        "    },\n",
        "    {\n",
        "        \"LEFT_ID\": \"verb\",\n",
        "        \"REL_OP\": \">\",\n",
        "        \"RIGHT_ID\": \"attribute\",\n",
        "        \"RIGHT_ATTRS\": {\"POS\":\"NOUN\",\"DEP\":\"dobj\"}\n",
        "    }\n",
        "    ]\n",
        "#The pattern is added to the matcher_1 using matcher_1.add(\"pattern_1\", [pattern_1]).\n",
        "    matcher_1.add(\"pattern_1\", [pattern_1])\n",
        "\n",
        "#The sentence is processed using nlp to obtain a spaCy doc object.\n",
        "    doc = nlp(sent)\n",
        "#The matcher_1 is applied to the doc using matcher_1(doc) to find matches.\n",
        "    matches = matcher_1(doc)\n",
        "\n",
        "    if(matches):\n",
        "# Each token_id corresponds to one pattern dict\n",
        "        match_id, token_ids = matches[0]\n",
        "#The function returns the text of the first token (doc[token_ids[0]].text) that matches the pattern i.e verb\n",
        "        return doc[token_ids[0]].text\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "LiOQCEAEiLK_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93c5ce19-8a32-42a0-e62a-c805644f9840"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "has\n"
          ]
        }
      ],
      "source": [
        "# Example sentence\n",
        "sentence = \"John has a car.\"\n",
        "\n",
        "# Apply the pattern_1 function\n",
        "result = pattern_1(sentence)\n",
        "\n",
        "# Print the result\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "7w42zoJciwIn"
      },
      "outputs": [],
      "source": [
        "'''The spacy_noun function takes a sentence as input and uses spaCy to extract noun phrases (noun chunks) from the sentence.\n",
        "It returns a list of the extracted noun phrases.'''\n",
        "def spacy_noun(sentence):\n",
        "  doc=nlp(sentence)\n",
        "  result=[]\n",
        "  for np in doc.noun_chunks:\n",
        "        result.append(np.text)\n",
        "  return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "6UKK8cjtjl8A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e21a0d64-8162-41d6-9cd3-c0bef712b4b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The quick brown fox\n",
            "the lazy dog\n"
          ]
        }
      ],
      "source": [
        "# Example sentence\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# Apply the spacy_noun function\n",
        "noun_phrases = spacy_noun(sentence)\n",
        "\n",
        "# Print the extracted noun phrases\n",
        "for noun_phrase in noun_phrases:\n",
        "    print(noun_phrase)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "-c_5sy95jorA"
      },
      "outputs": [],
      "source": [
        "'''The spacy_ner function takes a sentence as input and uses spaCy to extract named entities from the sentence.\n",
        "It returns a list of the extracted named entities, excluding entities labeled as \"CARDINAL\" (cardinal numbers).'''\n",
        "def spacy_ner(sentence):\n",
        "  doc=nlp(sentence)\n",
        "  result=[]\n",
        "  for ent in doc.ents:\n",
        "        if (ent.label_!='CARDINAL'):\n",
        "            result.append(ent.text)\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "XCyMLTzwj3sv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b453d8ad-35ad-4066-abed-a3c020eafaac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple Inc.\n",
            "Steve Jobs\n",
            "April 1976\n"
          ]
        }
      ],
      "source": [
        "# Example sentence\n",
        "sentence = \"Apple Inc. was founded by Steve Jobs in April 1976.\"\n",
        "\n",
        "# Apply the spacy_ner function\n",
        "named_entities = spacy_ner(sentence)\n",
        "\n",
        "# Print the extracted named entities\n",
        "for named_entity in named_entities:\n",
        "    print(named_entity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "gpz4_jJ2j4PM"
      },
      "outputs": [],
      "source": [
        "'''The no_nouns function takes four parameters: ent, rel, quant phrases and match.\n",
        "It performs several checks and returns a boolean value based on the conditions specified within the function.'''\n",
        "\n",
        "def no_nouns(ent,rel,quant,match):\n",
        "\n",
        "  ent,rel,quant,match1=ent,rel,quant,match\n",
        "  #If the string \" s\", \" FM\", or the unit name of the first item in match is \"dimensionless\", the function returns False\n",
        "  if(\" s\" in quant or \" FM\" in quant or match1[0].unit.name=='dimensionless' ):\n",
        "    return False\n",
        "  result=[]\n",
        "  ent_match=parser.parse(ent)\n",
        "  #return false if entity has any dimension quantity\n",
        "  for m in ent_match:\n",
        "    if(m.unit.name!='dimensionless'):\n",
        "      return False\n",
        "  ent=nlp(ent)\n",
        "  #check for entities present in entity phrase\n",
        "  for ent in ent.ents:\n",
        "           if (ent.label_!='CARDINAL'):\n",
        "                 result.append(ent.text)\n",
        "  #if there is no entity check if there exist either a proper noun or noun in entity phrase\n",
        "  if(not(result)):\n",
        "    for t in ent:\n",
        "       if(t.pos_=='PROPN' or t.pos_=='NOUN'):\n",
        "               result=t.text\n",
        "  #if there is no entity , no proper noun , no noun in entity phrase return false\n",
        "  if(not(result)):\n",
        "    return False\n",
        "  #return false if there exists noun in either relation or quantity phrase\n",
        "  start,end=match1[0].span\n",
        "\n",
        "  q=quant[start:end]\n",
        "\n",
        "  quant=quant.replace(q,'')\n",
        "  rel=nlp(rel)\n",
        "  quant=nlp(quant)\n",
        "\n",
        "  for t in quant:\n",
        "    if(t.pos_=='NOUN'):\n",
        "      return False\n",
        "\n",
        "  for t in rel:\n",
        "    if(t.pos_=='NOUN'):\n",
        "      return False\n",
        "  #return true if there is atleast one entity/propernoun/noun in entity and no nouns in either relation or quantity phrase\n",
        "  return True\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VA63SYbnjnP",
        "outputId": "4077f288-1f71-44d4-bf5a-5b2b4c288583"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ],
      "source": [
        "match1=parser.parse('10 million')\n",
        "# Example values for the function parameters\n",
        "ent = \"Apple Inc.\"\n",
        "rel = \"is a company.\"\n",
        "quant = \"10 million\"\n",
        "\n",
        "\n",
        "# Apply the no_nouns function\n",
        "result = no_nouns(ent, rel, quant, match1)\n",
        "\n",
        "# Print the result\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S88GWoj9oF_7",
        "outputId": "fffc830e-601d-4f6d-fe49-130b47c86afe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "match1=parser.parse('8 years')\n",
        "# Example values for the function parameters\n",
        "ent = \"John\"\n",
        "rel = \"is\"\n",
        "quant = \"8 years\"\n",
        "\n",
        "\n",
        "# Apply the no_nouns function\n",
        "result = no_nouns(ent, rel, quant, match1)\n",
        "\n",
        "# Print the result\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "-QsZZawoqEjJ"
      },
      "outputs": [],
      "source": [
        "#The noun_chunks function takes a text input and extracts noun phrases from it using predefined patterns.\n",
        "def noun_chunks(text):\n",
        "  doc = nlp(text)\n",
        "  pattern = [\n",
        "      #extract noun phrases that consist of optional adjectives followed by one or more nouns like big red apple\n",
        "      [{\"POS\": \"ADJ\",\"op\":\"*\"},{\"POS\": \"NOUN\",\"op\":\"+\"}],\n",
        "#extract noun phrases that can consist of optional adjectives, followed by one or more nouns, optional particles (e.g., prepositions),\n",
        "#and additional optional adjectives and nouns like \"Incredible India's GDP\"\n",
        "\n",
        "\n",
        "            [{\"POS\": \"ADJ\",\"op\":\"*\"},{\"POS\": \"NOUN\",\"op\":\"+\"},{\"POS\": \"PART\",\"op\":\"*\"},{\"POS\": \"ADJ\",\"op\":\"*\"},{\"POS\": \"NOUN\",\"op\":\"*\"}],\n",
        "#optional adjectives, followed by one or more proper nouns, optional particles, and additional optional proper nouns like New York City\n",
        "            [{\"POS\": \"ADJ\",\"op\":\"*\"},{\"POS\": \"PROPN\",\"op\":\"+\"},{\"POS\": \"PART\",\"op\":\"*\"},{\"POS\": \"PROPN\",\"op\":\"*\"}],\n",
        "#optional adjectives, followed by one or more proper nouns, optional particles, additional optional adjectives, and optional nouns like New York\n",
        "\n",
        "\n",
        "\n",
        "             [{\"POS\": \"ADJ\",\"op\":\"*\"},{\"POS\": \"PROPN\",\"op\":\"+\"},{\"POS\": \"PART\",\"op\":\"*\"},{\"POS\": \"ADJ\",\"op\":\"*\"},{\"POS\": \"NOUN\",\"op\":\"*\"}]\n",
        "            ]\n",
        "  matcher = Matcher(nlp.vocab)\n",
        "  matcher.add('NOUN_PHRASE', pattern)\n",
        "  matches = matcher(doc)\n",
        "\n",
        "  spans = [doc[start:end] for match_id, start, end in matches]\n",
        "\n",
        "  return filter_spans(spans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "NIkFkKXtwpBO"
      },
      "outputs": [],
      "source": [
        "#The propern function takes a text as input, applies a pattern to extract noun phrases consisting of proper nouns, and returns the extracted noun phrases.\n",
        "def propern(text):\n",
        "  doc = nlp(text)\n",
        "  pattern = [\n",
        "      #optional adjectives, followed by one or more proper nouns, one or more particles, and additional optional proper nouns\n",
        "            [{\"POS\": \"ADJ\",\"op\":\"*\"},{\"POS\": \"PROPN\",\"op\":\"+\"},{\"POS\": \"PART\",\"op\":\"+\"},{\"POS\": \"PROPN\",\"op\":\"*\"}],\n",
        "      #optional adjectives, followed by one or more proper nouns, and optional particles\n",
        "             [{\"POS\": \"ADJ\",\"op\":\"*\"},{\"POS\": \"PROPN\",\"op\":\"+\"},{\"POS\": \"PART\",\"op\":\"*\"}]\n",
        "            ]\n",
        "  matcher = Matcher(nlp.vocab)\n",
        "  matcher.add('PNOUN_PHRASE', pattern)\n",
        "  matches = matcher(doc)\n",
        "\n",
        "  spans = [doc[start:end] for match_id, start, end in matches]\n",
        "\n",
        "  return filter_spans(spans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "OP5uqvVJyPce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2054e09f-eb92-4b70-ea5e-1ac219733bf8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Navya's]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "propern(\"Navya's home\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "1fNNef3x2pks"
      },
      "outputs": [],
      "source": [
        "#ner_noun function takes a text as input, applies a pattern to extract noun phrases that contain named entities, and returns the extracted noun phrases.\n",
        "def ner_noun(text):\n",
        "    doc = nlp(text)\n",
        "    pattern = [ #1\n",
        "               {\"POS\": \"DET\",\"op\":\"*\"}, {\"POS\": \"ADJ\",'op':'*'},\n",
        "      {\"POS\": \"PROPN\",\"op\":\"+\"},{\"POS\": \"PART\",\"op\":\"*\"},{\"POS\": \"PROPN\",\"op\":\"*\"}, {\"POS\": \"DET\",\"op\":\"*\"}, {\"POS\": \"ADJ\",'op':'*'},\n",
        "      {\"POS\": \"NOUN\",\"op\":\"*\"},{\"POS\": \"PART\",\"op\":\"*\"},\n",
        "      {\"POS\": \"NOUN\",\"op\":\"*\"},\n",
        "      #2\n",
        "   {\"POS\": \"DET\",\"op\":\"*\"},\n",
        "               {\"POS\": \"PROPN\",\"op\":\"*\"},{\"POS\": \"NOUN\",\"op\":\"*\"}]\n",
        "\n",
        "\n",
        "   #{\"POS\": \"ADP\",\"op\":\"*\"},\n",
        "\n",
        "\n",
        "    matcher_ner = Matcher(nlp.vocab)\n",
        "    matcher_ner.add(\"ner_pattern\", [pattern])\n",
        "    matches = matcher_ner(nlp(text))\n",
        "\n",
        "    spans = [doc[start:end] for match_id, start, end in matches]\n",
        "\n",
        "    return filter_spans(spans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "QkEVjT4t4-vr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0496f387-2676-4566-d5c7-48c7ee72e8e1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[The incredible India's GDP]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "ner_noun(\"The incredible India's GDP\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "WpyT2pPg6BOO"
      },
      "outputs": [],
      "source": [
        "#include phrases that starts with adj , proper noun , noun it helps in removing articles, pronouns etc\n",
        "def only_noun_adj(phrase):\n",
        "  phrase=phrase\n",
        "\n",
        "  d=nlp(phrase)\n",
        "  for t in d:\n",
        "    b=t.text\n",
        "    if(t.pos_=='ADJ' or t.pos_=='PROPN' or t.pos_=='NOUN'):\n",
        "      return phrase[phrase.index(b):]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "17gwYRcA6ugd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "f6345052-d439-49cf-c382-c3bdab20e2fe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'beautiful lake'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "only_noun_adj('the beautiful lake')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "JWaYRf8k61U3"
      },
      "outputs": [],
      "source": [
        "#defined to match entitites if one entity is connected by atmost 5 entitites like if we have only India in entity phrase but in sentence we have\n",
        "#India,china,Japan and Russia  present as total entity span\n",
        "#returns total entity span along with entitites in seperate list\n",
        "def entity_and(text):\n",
        "   doc = nlp(text)\n",
        "   pattern_ner=[]\n",
        "   for ii in range(1, 5):\n",
        "        pattern=[{\"POS\": \"DET\",\"op\":\"*\"}, {\"POS\": \"ADJ\",'op':'*'},{\"POS\": \"PROPN\",\"op\":\"+\"},{\"POS\": \"PART\",\"op\":\"*\"},{\"IS_PUNCT\":True,'op':'*'}]*ii\n",
        "        pattern+=[{\"LOWER\":'and'},{\"POS\": \"DET\",\"op\":\"*\"}, {\"POS\": \"ADJ\",'op':'*'},{\"POS\": \"PROPN\",\"op\":\"+\"},{\"POS\": \"PART\",\"op\":\"*\"}]\n",
        "\n",
        "        pattern_ner.append(pattern)\n",
        "\n",
        "\n",
        "\n",
        "   matcher = Matcher(nlp.vocab)\n",
        "   matcher.add('NER_pattern', pattern_ner)\n",
        "\n",
        "\n",
        "   matches = matcher(doc)\n",
        "\n",
        "   spans = [doc[start:end] for match_id, start, end in matches]\n",
        "\n",
        "\n",
        "   if(filter_spans(spans)):\n",
        "     entity_span=str(filter_spans(spans)[0])\n",
        "     doc = nlp(entity_span)\n",
        "     entities=[str(ent.text) for ent in doc.ents]\n",
        "     if(not(entities)):\n",
        "       for t in doc:\n",
        "         if(t.pos_=='PROPN'):\n",
        "           entities.append(t.text)\n",
        "     return entity_span,entities\n",
        "   else:\n",
        "    return '',[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "brvkYTXE9duT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0974abca-48e4-4ea1-eab2-13fa6c27b415"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('India,china,Japan and Russia', ['India', 'china', 'Japan', 'Russia'])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "entity_and('India,china,Japan and Russia are the 59% contributors for growth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "VN263_mE88wz"
      },
      "outputs": [],
      "source": [
        "#defined to match entitites if one entity is connected by atmost 4 entitites and followed by noun like if we have only India in entity phrase but in sentence we have\n",
        "#India,china,Japan and other countries present as total entity span\n",
        "#returns total entity span along with entitites in seperate list\n",
        "def entity_and_noun(text):\n",
        "   doc = nlp(text)\n",
        "\n",
        "\n",
        "   pattern_ner=[]\n",
        "   for ii in range(1, 5):\n",
        "        pattern=[{\"POS\": \"DET\",\"op\":\"*\"}, {\"POS\": \"ADJ\",'op':'*'},{\"POS\": \"PROPN\",\"op\":\"+\"},{\"POS\": \"PART\",\"op\":\"*\"},{\"IS_PUNCT\":True,'op':'*'}]*ii\n",
        "        pattern+=[{\"LOWER\":'and'},{\"POS\": \"DET\",\"op\":\"*\"}, {\"POS\": \"ADJ\",'op':'*'},{\"POS\": \"NOUN\",\"op\":\"+\"},{\"POS\": \"PART\",\"op\":\"*\"}]\n",
        "\n",
        "        pattern_ner.append(pattern)\n",
        "\n",
        "   matcher = Matcher(nlp.vocab)\n",
        "   matcher.add('NER_pattern', pattern_ner)\n",
        "   matches = matcher(doc)\n",
        "\n",
        "   spans = [doc[start:end] for match_id, start, end in matches]\n",
        "   if(filter_spans(spans)):\n",
        "     entity_span=str(filter_spans(spans)[0])\n",
        "     doc = nlp(entity_span)\n",
        "     entities=[str(ent.text) for ent in doc.ents]\n",
        "     if(not(entities)):\n",
        "       for t in doc:\n",
        "         if(t.pos_=='PROPN'):\n",
        "           entities.append(t.text)\n",
        "     return entity_span,entities\n",
        "\n",
        "   else:\n",
        "    return '',[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "ayRvYof59o1u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04d30597-1e66-43f2-e3c2-174b01d95abe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('India,china,Japan and other countries', ['India', 'china', 'Japan'])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "entity_and_noun('India,china,Japan and other countries are the 59% contributors for growth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "0Sar-R0T_NGW"
      },
      "outputs": [],
      "source": [
        "#pattern that matches a sequence of tokens with optional adjectives, proper nouns, optional particles, optional adjectives, optional nouns, followed by the word \"to,\" and\n",
        "#another sequence of tokens with optional determiners, adjectives, proper nouns, optional particles, optional adjectives, and optional nouns.\n",
        "def entity_to(text):\n",
        "   doc = nlp(text)\n",
        "\n",
        "\n",
        "   pattern_ner=[]\n",
        "\n",
        "   pattern=[{\"POS\": \"ADJ\",'op':'*'},{\"POS\": \"PROPN\",\"op\":\"+\"},{\"POS\": \"PART\",\"op\":\"*\"},{\"POS\": \"ADJ\",'op':'*'},{\"POS\":'NOUN','op':'*'},\n",
        "        {\"LOWER\":'to'},{\"POS\": \"DET\",'op':'*'},{\"POS\": \"ADJ\",'op':'*'},{\"POS\": \"PROPN\",\"op\":\"+\"},{\"POS\": \"PART\",\"op\":\"*\"},{\"POS\": \"ADJ\",'op':'*'},{\"POS\":'NOUN','op':'*'}]\n",
        "   pattern_ner.append(pattern)\n",
        "\n",
        "   matcher = Matcher(nlp.vocab)\n",
        "   matcher.add('NER_pattern', pattern_ner)\n",
        "   matches = matcher(doc)\n",
        "\n",
        "   spans = [doc[start:end] for match_id, start, end in matches]\n",
        "   if(filter_spans(spans)):\n",
        "     entity_span=str(filter_spans(spans)[0])\n",
        "     return entity_span\n",
        "   else:\n",
        "     return ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "loI-NF2b_23J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "a7b34fea-105e-4468-a22c-7ab96c90142c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'India to China'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "entity_to('The distance to India to China is 2000 km')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "tGJmL7RqAFRZ"
      },
      "outputs": [],
      "source": [
        "#function takes sentence and quantity phrase and checks if there is any 'of' phrase related to quantity like\n",
        "#the population of India is 30 % of china's population. where 30 % is related to china's population, we will return of china population as output\n",
        "def prep(text,quant):\n",
        "  doc=nlp(text)\n",
        "  quant=quant\n",
        "  q=''\n",
        "  n=''\n",
        "  for token in doc:\n",
        "       if(token.dep_=='nummod' and token.text in quant and token.head.text in quant):\n",
        "         q=token.head.text\n",
        "\n",
        "  for token in doc:\n",
        "\n",
        "    if(token.text=='of' and token.head.text==q):\n",
        "\n",
        "      for child in token.children:\n",
        "\n",
        "        if(child.pos_ in ['NOUN','PROPN']):\n",
        "\n",
        "            n=\"of \"+\" \".join([c.text for c in child.lefts])+\" \"+str(child)\n",
        "            return n\n",
        "  return n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "w2ibv7l3A0xo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98c2830a-bb18-4f16-ecbe-5bbbf02d05d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "of china population\n"
          ]
        }
      ],
      "source": [
        "text = \"The population of India is 30 % of china's population.\"\n",
        "quant = \"30 %\"\n",
        "\n",
        "result = prep(text, quant)\n",
        "\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To extract noun phrases that are related to a quantity by the word \"of, defined three patterns percentage_relation_1, percentage_relation_2, percentage_relation_3"
      ],
      "metadata": {
        "id": "COMuCRGjs_OX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "-p6jvIPMEmuD"
      },
      "outputs": [],
      "source": [
        "def percentage_relation_1(text):\n",
        "  doc = nlp(text)\n",
        "  pattern = [\n",
        "      [{'ORTH':'of'},{'IS_ASCII':True,'ORTH':'<'},{'ORTH':'q'},{'IS_ASCII':True,'ORTH':'>'},{'ORTH':'of'},{\"POS\": \"DET\",\"op\":\"*\"},\n",
        "       {\"POS\": \"ADJ\",\"op\":\"*\"},{\"POS\": \"NOUN\",\"op\":\"+\"},{\"POS\": \"PART\",\"op\":\"*\"},{\"POS\": \"DET\",\"op\":\"*\"},{\"POS\": \"ADJ\",\"op\":\"*\"},{\"POS\": \"NOUN\",\"op\":\"*\"}],\n",
        "             [{'ORTH':'of'},{'IS_ASCII':True,'ORTH':'<'},{'ORTH':'q'},{'IS_ASCII':True,'ORTH':'>'},{'ORTH':'of'},\n",
        "              {\"POS\": \"DET\",\"op\":\"*\"},{\"POS\": \"ADJ\",\"op\":\"*\"},{\"POS\": \"PROPN\",\"op\":\"+\"},{\"POS\": \"PART\",\"op\":\"*\"},\n",
        "              {\"POS\": \"DET\",\"op\":\"*\"},{\"POS\": \"ADJ\",\"op\":\"*\"},{\"POS\": \"PROPN\",\"op\":\"*\"},\n",
        "              {\"POS\": \"NOUN\",\"op\":\"*\"},{\"POS\": \"PART\",\"op\":\"*\"},{\"POS\": \"NOUN\",\"op\":\"*\"}]\n",
        "            ]\n",
        "  matcher = Matcher(nlp.vocab)\n",
        "  matcher.add(\"percentage\", pattern)\n",
        "  matches = matcher(doc)\n",
        "\n",
        "  spans = [doc[start:end] for match_id, start, end in matches]\n",
        "\n",
        "  return filter_spans(spans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "7bDwrrYGFTfj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c743066-1e04-4ea2-89cf-d5540c2c8c13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[of <q> of china's population]\n"
          ]
        }
      ],
      "source": [
        "text = \"The population of India is of <q> of china's population.\"\n",
        "\n",
        "result = percentage_relation_1(text)\n",
        "\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "bHs46ofcEoIi"
      },
      "outputs": [],
      "source": [
        "def percentage_relation_2(text):\n",
        "  doc = nlp(text)\n",
        "  pattern = [\n",
        "      [{'IS_ASCII':True,'ORTH':'<'},{'ORTH':'q'},{'IS_ASCII':True,'ORTH':'>'},{\"POS\": \"DET\",\"op\":\"*\"},\n",
        "       {\"POS\": \"ADJ\",\"op\":\"*\"},{\"POS\": \"NOUN\",\"op\":\"+\"},{\"POS\": \"PART\",\"op\":\"*\"},{\"POS\": \"DET\",\"op\":\"*\"},\n",
        "       {\"POS\": \"ADJ\",\"op\":\"*\"},{\"POS\": \"NOUN\",\"op\":\"*\"},\n",
        "       {'ORTH':'of'},{\"POS\": \"DET\",\"op\":\"*\"},\n",
        "       {\"POS\": \"ADJ\",\"op\":\"*\"},{\"POS\": \"NOUN\",\"op\":\"+\"},{\"POS\": \"PART\",\"op\":\"*\"},{\"POS\": \"DET\",\"op\":\"*\"},\n",
        "       {\"POS\": \"ADJ\",\"op\":\"*\"},{\"POS\": \"NOUN\",\"op\":\"*\"}],\n",
        " [{'IS_ASCII':True,'ORTH':'<'},{'ORTH':'q'},{'IS_ASCII':True,'ORTH':'>'},{\"POS\": \"DET\",\"op\":\"*\"},\n",
        "       {\"POS\": \"ADJ\",\"op\":\"*\"},{\"POS\": \"PROPN\",\"op\":'+'},{\"POS\": \"PART\",\"op\":\"*\"},{\"POS\": \"DET\",\"op\":\"*\"},\n",
        "       {\"POS\": \"ADJ\",\"op\":\"*\"},{\"POS\": \"PROPN\",\"op\":'*'},\n",
        "  {\"POS\": \"NOUN\",\"op\":\"*\"},{\"POS\": \"PART\",\"op\":\"*\"},{\"POS\": \"NOUN\",\"op\":\"*\"},{'ORTH':'of'},{\"POS\": \"DET\",\"op\":\"*\"},\n",
        "       {\"POS\": \"ADJ\",\"op\":\"*\"},{\"POS\": \"PROPN\",\"op\":\"+\"},{\"POS\": \"PART\",\"op\":\"*\"},\n",
        "  {\"POS\": \"DET\",\"op\":\"*\"},\n",
        "       {\"POS\": \"ADJ\",\"op\":\"*\"},{\"POS\": \"PROPN\",\"op\":\"*\"},{\"POS\": \"NOUN\",\"op\":\"*\"},{\"POS\": \"PART\",\"op\":\"*\"},{\"POS\": \"NOUN\",\"op\":\"*\"}]\n",
        "            ]\n",
        "  matcher = Matcher(nlp.vocab)\n",
        "  matcher.add(\"percentage\", pattern)\n",
        "  matches = matcher(doc)\n",
        "\n",
        "  spans = [doc[start:end] for match_id, start, end in matches]\n",
        "\n",
        "  return filter_spans(spans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "ItU_RLRAHZAb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cf3fc47-7f33-423d-c4c1-ff16b6d6e585"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<q> savings of salary]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "percentage_relation_2('I do <q> savings of salary')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "88c1d2xHEqrw"
      },
      "outputs": [],
      "source": [
        "def percentage_relation_3(text):\n",
        "  doc = nlp(text)\n",
        "  pattern = [\n",
        "      [{'IS_ASCII':True,'ORTH':'<'},{'ORTH':'q'},{'IS_ASCII':True,'ORTH':'>'},{'ORTH':'of'},{\"POS\": \"DET\",\"op\":\"*\"},\n",
        "       {\"POS\": \"ADJ\",\"op\":\"*\"},{\"POS\": \"NOUN\",\"op\":\"+\"},{\"POS\": \"PART\",\"op\":\"*\"},{\"POS\": \"DET\",\"op\":\"*\"},{\"POS\": \"ADJ\",\"op\":\"*\"},{\"POS\": \"NOUN\",\"op\":\"*\"}],\n",
        "             [{'IS_ASCII':True,'ORTH':'<'},{'ORTH':'q'},{'IS_ASCII':True,'ORTH':'>'},{'ORTH':'of'},\n",
        "              {\"POS\": \"DET\",\"op\":\"*\"},{\"POS\": \"ADJ\",\"op\":\"*\"},{\"POS\": \"PROPN\",\"op\":\"+\"},{\"POS\": \"PART\",\"op\":\"*\"},\n",
        "              {\"POS\": \"DET\",\"op\":\"*\"},{\"POS\": \"ADJ\",\"op\":\"*\"},{\"POS\": \"PROPN\",\"op\":\"*\"},\n",
        "              {\"POS\": \"NOUN\",\"op\":\"*\"},{\"POS\": \"PART\",\"op\":\"*\"},{\"POS\": \"NOUN\",\"op\":\"*\"}]\n",
        "            ]\n",
        "  matcher = Matcher(nlp.vocab)\n",
        "  matcher.add(\"percentage\", pattern)\n",
        "  matches = matcher(doc)\n",
        "\n",
        "  spans = [doc[start:end] for match_id, start, end in matches]\n",
        "\n",
        "  return filter_spans(spans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "sx_LcmDqJO9Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0af4ec6c-415f-4f2d-8407-8736541f1730"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<q> of salary]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "percentage_relation_3('I do <q> of salary as savings')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "eBFfPS3hiK02"
      },
      "outputs": [],
      "source": [
        "#extract noun related to quantity if any\n",
        "def extract_np_quantity(doc):\n",
        "\n",
        "    for token in doc:\n",
        "        if token.text == \"QUANTITY\":\n",
        "            for child in token.children:\n",
        "                if child.pos_ == \"NOUN\":\n",
        "                    return child.text\n",
        "                for child_of_child in child.children:\n",
        "                    if child_of_child.pos_ == \"NOUN\":\n",
        "                        return child_of_child.text\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "Yt_vej0CiQrf"
      },
      "outputs": [],
      "source": [
        "#extract the total noun phrase related to quantity if any by checking if any noun related to quantity\n",
        "def np_noun(doc):\n",
        "  np=extract_np_quantity(doc)\n",
        "  if(np):\n",
        "\n",
        "      total_noun_phrases=[chunk.text for chunk in doc.noun_chunks]\n",
        "      for req_n in total_noun_phrases:\n",
        "             if(np in req_n):\n",
        "               req=[]\n",
        "               for t in nlp(req_n):\n",
        "                 if(t.pos_ in ['NOUN','PROPN','ADJ']):\n",
        "                       req.append(t.text)\n",
        "               return \" \".join(req)\n",
        "      return None\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "VbMbqe8tDQIg"
      },
      "outputs": [],
      "source": [
        "#matches for noun followed by preposition followed by noun\n",
        "def pattern_en_1(sent):\n",
        "    matcher_1 = DependencyMatcher(nlp.vocab)\n",
        "\n",
        "    pattern_1 = [\n",
        "    {\n",
        "        \"RIGHT_ID\": \"prep\",\n",
        "        \"RIGHT_ATTRS\": {\"POS\": \"ADP\"} #\"ORTH\": {\"IN\":[\"HAS\",\"HAD\",\"HAVE\"]}\n",
        "    },\n",
        "    {\n",
        "        \"LEFT_ID\": \"prep\",\n",
        "        \"REL_OP\": \"<\",\n",
        "        \"RIGHT_ID\": \"noun1\",\n",
        "\n",
        "        \"RIGHT_ATTRS\": {\"POS\": \"NOUN\"},\n",
        "    },\n",
        "    {\n",
        "        \"LEFT_ID\": \"prep\",\n",
        "        \"REL_OP\": \">\",\n",
        "        \"RIGHT_ID\": \"NOUN2\",\n",
        "        \"RIGHT_ATTRS\": {\"POS\":\"NOUN\"}\n",
        "    }\n",
        "\n",
        "    ]\n",
        "\n",
        "    matcher_1.add(\"pattern_1\", [pattern_1])\n",
        "    doc = nlp(sent)\n",
        "    doc = merge_phrases(doc)\n",
        "    matches = matcher_1(doc)\n",
        "\n",
        "    if(matches):\n",
        "# Each token_id corresponds to one pattern dict\n",
        "        match_id, token_ids = matches[0]\n",
        "\n",
        "        return doc[token_ids[1]].text+\" \"+doc[token_ids[0]].text+\" \"+doc[token_ids[2]].text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "9yGrMr-DJftQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "04bb6cce-9e1f-4c2c-d84e-922420147326"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The population of city'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "pattern_en_1('The population of city')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "71fK5eR1J95a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "8e66f24d-beaf-45f4-ceea-da19e31baaf9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The sculpture in park'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "pattern_en_1('The sculpture in park')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "Z21IZTbaKEVF"
      },
      "outputs": [],
      "source": [
        "#The function pos_word takes a word as input and returns its part-of-speech (POS) tag using SpaCy.\n",
        "\n",
        "\n",
        "def pos_word(word):\n",
        "  doc=nlp(word)\n",
        "  for to in doc:\n",
        "    return to.pos_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IPJ_WXlN5Iad"
      },
      "outputs": [],
      "source": [
        "#suppose in sentence, quantity is like 9km(9000m) , quantulum parser detects 9km as quantity. As we are considering the other quantities with in specified\n",
        "#window length for 'range quantities' , it shouldn't consider 9000m as seperate quantity. AS we are replacing quantity <q> , the total 9km(9000m) should masked\n",
        "#as <q>. for this we defined this function\n",
        "def quant_paren(sent):\n",
        "  k=sent\n",
        "  e=sent\n",
        "  q_list=[]\n",
        "  total_q=parser.parse(e)\n",
        "  for i in range(len(total_q)):\n",
        "     s,e1=total_q[i].span\n",
        "     mat_q=k[s:e1]\n",
        "     #for 'in' unit which is inches ,(9 in) quantulum detects 9 as quantity instead of 9 in . so we included this special case condition\n",
        "     mat_q_inch=mat_q+ \" in\"\n",
        "     if(total_q[i].unit.name!='dimensionless' ):\n",
        "        e=e.replace(mat_q,'<q1>')\n",
        "        q_list.append(mat_q)\n",
        "     elif(mat_q_inch in e):\n",
        "        e=e.replace(mat_q_inch,'<q1>')\n",
        "        q_list.append(mat_q_inch)\n",
        "  e=e.replace('( <','(<')\n",
        "  e=e.replace('> )','>)')\n",
        "  e=e.replace('q> (','q>(')\n",
        "  if('<q>(<q1>)' in e):\n",
        "     e=e.replace('<q>(<q1>)','<q>')\n",
        "     first,second=e.split('<q>')[0],e.split('<q>')[1]\n",
        "     first_count=first.count('<q1>')\n",
        "\n",
        "     for i in range(first_count):\n",
        "       first=first.replace('<q1>',q_list[i],1)\n",
        "     q_list=q_list[first_count+1:]\n",
        "\n",
        "     for i in range(len(q_list)):\n",
        "      second=second.replace('<q1>',q_list[i],1)\n",
        "     return first+\"<q>\"+second\n",
        "  else:\n",
        "    return k\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKE8tm3gDTCg"
      },
      "outputs": [],
      "source": [
        "#return entity , defined for distance attributes.\n",
        "\n",
        "def entity_attribute_spl(ent_phrase):\n",
        "  ent_phrase=ent_phrase\n",
        "  #find entities in entity phrase\n",
        "  ents=spacy_ner(ent_phrase)\n",
        "  #find nouns\n",
        "  np=spacy_noun(ent_phrase)\n",
        "  np_dup=spacy_noun(ent_phrase)\n",
        "  ner_nouns=[]\n",
        "  #storing entity nouns and removing entities from np list\n",
        "  for e in ents:\n",
        "    for n in np:\n",
        "      if(str(e) in str(n)):\n",
        "        if(n in np_dup):\n",
        "            np_dup.remove(n)\n",
        "        ner_nouns.append(n)\n",
        "  #found atleast one entity\n",
        "  if(ents):\n",
        "    #if only one noun phrase in entity phrase\n",
        "    if(len(np_dup)==1):\n",
        "      #check whether it is in format of 'noun of entity' like bus station of palakkad\n",
        "      e=str(np_dup[0]) +\" of \"+str(ents[0])\n",
        "      #if yes,return total as entity\n",
        "      if(e in str(ent_phrase)):\n",
        "        #only_noun_adj returns the noun phrase by removing articles, pronouns etc. like if you pass 'The bus station of palakkad', it returns bus station of palakkad\n",
        "        return only_noun_adj(e)\n",
        "      else:\n",
        "        #else find the first entity in actual nouns since some entities will be like china's railway station where ner detects only china\n",
        "        for n in ner_nouns:\n",
        "          if(str(ents[0]) in str(n)):\n",
        "                return only_noun_adj(str(n))\n",
        "        #nthg above  matches  means the entity is not in above formats , so return first entity\n",
        "        return str(ents[0])\n",
        "   #if we have more than one nouns in entity phrase\n",
        "    elif(len(np_dup)>1):\n",
        "      e=str(np_dup[0]) +\" of \"+str(ents[0])\n",
        "      #check whether it is in format of noun of entity like bus station of palakkad\n",
        "      if(e in str(ent_phrase)):\n",
        "        return only_noun_adj(e)\n",
        "    #if only one entity in entity phrase\n",
        "    elif(len(ents)==1):\n",
        "      #only one noun phrase and find the first entity in actual nouns since some entities will be like china's railway station where ner detects only china\n",
        "      if(len(np)==1 and str(ents[0]) in str(np[0])):\n",
        "        return only_noun_adj(str(np[0]))\n",
        "      #not matched , return the entity\n",
        "      return str(ents[0])\n",
        "    #else check in other nouns , whether ner entity is present in them, if present return entire noun phrase containing entity as required entity\n",
        "    else:\n",
        "      for n in ner_nouns:\n",
        "          if(str(ents[0]) in str(n)):\n",
        "                return only_noun_adj(str(n))\n",
        "\n",
        "      return str(ents[0])\n",
        "  #if no entities found , return the last noun phrase as required entity\n",
        "  else:\n",
        "    doc=nlp(ent_phrase)\n",
        "    total_noun_phrases=[chunk.text for chunk in doc.noun_chunks]\n",
        "    if(total_noun_phrases):\n",
        "       req_n=total_noun_phrases[-1]\n",
        "       req=[]\n",
        "       for t in nlp(req_n):\n",
        "                 if(t.pos_ in ['NOUN','PROPN','ADJ']):\n",
        "                       req.append(t.text)\n",
        "       return \" \".join(req)\n",
        "\n",
        "    return ''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzEsAGw4C9yt"
      },
      "outputs": [],
      "source": [
        "#defined to detect proper nouns like china's railway station\n",
        "def ner_noun_part(text):\n",
        "    doc = nlp(text)\n",
        "    pattern_part = [ #1\n",
        "               {\"POS\": \"DET\",\"op\":\"*\"}, {\"POS\": \"ADJ\",'op':'*'},\n",
        "      {\"POS\": \"PROPN\",\"op\":\"+\"},{\"POS\": \"PART\",\"op\":\"+\"},{\"POS\": \"PROPN\",\"op\":\"*\"}, {\"POS\": \"DET\",\"op\":\"*\"}, {\"POS\": \"ADJ\",'op':'*'},\n",
        "      {\"POS\": \"NOUN\",\"op\":\"*\"},{\"POS\": \"PART\",\"op\":\"*\"},\n",
        "      {\"POS\": \"NOUN\",\"op\":\"*\"},\n",
        "      ]\n",
        "\n",
        "\n",
        "   #{\"POS\": \"ADP\",\"op\":\"*\"},\n",
        "\n",
        "\n",
        "    matcher_ner_part = Matcher(nlp.vocab)\n",
        "    matcher_ner_part.add(\"ner_pattern\", [pattern_part])\n",
        "    matches = matcher_ner_part(nlp(text))\n",
        "\n",
        "    spans = [doc[start:end] for match_id, start, end in matches]\n",
        "\n",
        "    return filter_spans(spans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eN6oMScu58Fl"
      },
      "outputs": [],
      "source": [
        "#defined function to get entity and attribute for pattern 2\n",
        "def entity_attribute(ent_phrase,actual_sent):\n",
        "  ent_phrase,actual_sent=ent_phrase,actual_sent\n",
        "  req_en,attribute='',''\n",
        "  doc = nlp(ent_phrase)\n",
        "  matches=parser.parse(actual_sent)\n",
        "  result=[]\n",
        "  #find ner entities in entity phrase\n",
        "  for ent in doc.ents:\n",
        "        if (ent.label_!='CARDINAL'):\n",
        "               result.append(ent.text)\n",
        "  req_en_span=''\n",
        "  #find if entity span present\n",
        "  entity_span,entities=entity_and(actual_sent)\n",
        "\n",
        "  if( entity_span and len(matches)==1):\n",
        "\n",
        "        if(any(e in entities for e in result)):\n",
        "\n",
        "                     req_en_span=str(entity_span)\n",
        "\n",
        "  entity_span,entities=entity_and_noun(actual_sent)\n",
        "  if( entity_span and len(matches)==1):\n",
        "             if(any(e in entities for e in result)):\n",
        "\n",
        "                      req_en_span=str(entity_span)\n",
        "  # find to in entity is present\n",
        "  req_en_to=entity_to(str(ent_phrase)+\" \")\n",
        "  #for 's in entity\n",
        "  if(ner_noun_part(ent_phrase)):\n",
        "       #check for all three entity span conditions\n",
        "       if(req_en_span):\n",
        "         req_en=req_en_span\n",
        "         req_en=req_en.replace(\"' s\",\"'s\")\n",
        "         req_en=req_en.replace(\" 's\",\"'s\")\n",
        "       elif(req_en_to):\n",
        "         req_en=req_en_to\n",
        "         req_en=req_en.replace(\"' s\",\"'s\")\n",
        "         req_en=req_en.replace(\" 's\",\"'s\")\n",
        "       else:\n",
        "        #find first proper noun phrase assign as enity\n",
        "            req_en=str(ner_noun_part(ent_phrase)[0])\n",
        "            req_en=req_en.replace(\"' s\",\"'s\")\n",
        "            req_en=req_en.replace(\" 's\",\"'s\")\n",
        "      #if of is present then the sample example be like Income of John's mother\n",
        "       if('of' not in str(ent_phrase) and not(req_en_span) and not(req_en_to)):\n",
        "         req_en=req_en.replace(\"' s\",\"'s\")\n",
        "         req_en=req_en.replace(\" 's\",\"'s\")\n",
        "         #remove proper noun in entity phrase and assign as attribute like in china's GDP, assign GDP AS attribute\n",
        "         attribute=req_en.replace(str(propern(req_en)[0]),'')\n",
        "         #assign above removed phrase as required entity\n",
        "         req_en=str(propern(req_en)[0])\n",
        "       elif('at' in str(ent_phrase) and not(req_en_span) and not(req_en_to)):\n",
        "\n",
        "           req_en=req_en.replace(\"' s\",\"'s\")\n",
        "           req_en=req_en.replace(\" 's\",\"'s\")\n",
        "           #for example, 'Pollution at Delhi's palace entity phrase, return pollution as attribute and entity as Delhi's palace\n",
        "           req_e=spacy_ner(req_en)[0]\n",
        "\n",
        "\n",
        "           attribute=req_en.replace(str(propern(req_en)[0]),'')\n",
        "           if(req_e):\n",
        "             req_e=req_e.strip()\n",
        "           if(attribute):\n",
        "             attribute=attribute.strip()\n",
        "           return req_e,attribute\n",
        "      #find nouns which are not proper nouns found in above cases (so req_e will not be in nps list as it has a proper noun)\n",
        "       nps=[str(m) for m in noun_chunks(ent_phrase) if str(m) not in req_en]\n",
        "      #assign  first noun as atribute\n",
        "       if(nps):\n",
        "         for k in range(len(nps)-1,-1,-1):\n",
        "           if(not(propern(nps[k]))):\n",
        "\n",
        "             attribute=nps[k]\n",
        "\n",
        "\n",
        "       if(req_en):\n",
        "             req_en=str(req_en).strip()\n",
        "       if(attribute):\n",
        "             attribute=str(attribute).strip()\n",
        "       #if any of two , nothing found we return empty strings\n",
        "       return req_en,attribute\n",
        "  # 's is not in entity\n",
        "  else:\n",
        "        #find nouns and ner entities\n",
        "        nouns,ner=spacy_noun(ent_phrase),spacy_ner(ent_phrase)\n",
        "\n",
        "        #if only one noun found, assign that as entity,return empty string as attribute\n",
        "        if(len(nouns)==1):\n",
        "          #find wheher it is entity\n",
        "          en=str(spacy_ner(nouns[0]))\n",
        "          #if that is not ner entity , assign noun as entity, if its assign entity we got from ner\n",
        "          if not(en):\n",
        "            en=nouns[0]\n",
        "          else:\n",
        "            en=en[0]\n",
        "#check for entity spans\n",
        "          if(req_en_span):\n",
        "            req_en=req_en_span\n",
        "          elif(req_en_to):\n",
        "            req_en=req_en_to\n",
        "          elif(en):\n",
        "            req_en=str(nouns[0])\n",
        "          attribute=''\n",
        "\n",
        "          req_en=only_noun_adj(req_en)\n",
        "          if(req_en):\n",
        "            req_en=req_en.strip()\n",
        "          attribute=only_noun_adj(attribute)\n",
        "          if(attribute):\n",
        "            attribute=attribute.strip()\n",
        "          return req_en,attribute\n",
        "          #if more than one nouns, assign the first ner as entity and first noun as attribute\n",
        "        elif(len(nouns)>1):\n",
        "          #find nouns and ner\n",
        "           nouns,ner,ner_dup,nouns_dup=list(reversed(nouns)),list(reversed(ner)),list(reversed(ner)),list(reversed(nouns))\n",
        "           #remove those ners which has adj , adv as pos from ner list\n",
        "           for n in ner_dup:\n",
        "\n",
        "             if(pos_word(str(n)) in ['ADJ','ADV']):\n",
        "                   ner.remove(n)\n",
        "\n",
        "\n",
        "          #remove those nouns which are ners in nouns list\n",
        "           for ner_i in ner:\n",
        "              for np in nouns:\n",
        "                  if(str(ner_i) in str(np)):\n",
        "                          #first ner phrase will be the req_en\n",
        "                          req_en=only_noun_adj(str(np))\n",
        "                          if(np in nouns_dup):\n",
        "                           nouns_dup.remove(np)\n",
        "          #remove those nouns which are proper nouns\n",
        "           nouns_dup=[n for n in nouns_dup if pos_word(n)!='PROPN']\n",
        "\n",
        "           if(req_en_span):\n",
        "             req_en=req_en_span\n",
        "           if(req_en_to):\n",
        "             req_en=req_en_to\n",
        "\n",
        "           if(req_en):\n",
        "            req_en=req_en.strip()\n",
        "\n",
        "           if(nouns_dup):\n",
        "                 attribute=only_noun_adj(nouns_dup[-1])\n",
        "           else:\n",
        "              attribute=''\n",
        "\n",
        "           if(attribute):\n",
        "            attribute=attribute.strip()\n",
        "           return req_en,attribute\n",
        "\n",
        "\n",
        "        return req_en,attribute\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgUl_UlDx0Pg",
        "outputId": "131c9959-6b35-427f-b90b-3a715240a5f3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "adj_noun={\"old\":\"age\",\"long\":\"length\",\"longer\":\"length\",\"wide\":\"width\",\"wider\":\"width\",\"high\":\"height\",\"older\":\"age\",\"tall\":'height',\"taller\":'height'}\n",
        "dist_verbs=['situated','located','lies','stretches','passing']\n",
        "dist_verbs=[lemmatizer.lemmatize(i) for i in dist_verbs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4k7ggN6vpY4"
      },
      "outputs": [],
      "source": [
        "# output for pattern 2\n",
        "def ouput2(ent,rel,quant,mat,org_sent):\n",
        "\n",
        " ent,rel,quant,mat,org_sent=ent,rel,quant,mat,org_sent\n",
        " try:\n",
        "   req_relation=''\n",
        "\n",
        "   #for range quantity\n",
        "   ''' In cases where there is another quantity (q2) present within a window of length 3 from the quantity identified by the parser in the sentence,\n",
        "    and if the tokens ’to’ or ’-’ are present within this window,\n",
        "   we consider the final quantity to be a concatenation of the quantity obtained by the parser, the token ’to’ or ’-’, and the quantity q2 that is present within the window.\n",
        "   For example , ‘The flight duration has been increased from 2.5 hours to maximum of 3 hours.’ , the more appropriate quantity is 2.5 hours to 3 hours instead of 2.5 hours.'''\n",
        "   s,e=mat[0].span\n",
        "   q=quant[s:e]\n",
        "\n",
        "   req_quant=q\n",
        "\n",
        "   prep_of=prep(org_sent,req_quant)\n",
        "\n",
        "   e=org_sent.replace(q,'<q>')\n",
        "   e=quant_paren(e)\n",
        "   e=e[e.index('<q>'):]\n",
        "   start_q=e[e.index('<q>'):]\n",
        "   total_q=parser.parse(e)\n",
        "   q_list=[]\n",
        "\n",
        "   for i in range(len(total_q)):\n",
        "     s,e1=total_q[i].span\n",
        "     mat_q=e[s:e1]\n",
        "\n",
        "     if(total_q[i].unit.name!='dimensionless' and 's' not in mat_q ):\n",
        "        e=e.replace(mat_q,'<q1>')\n",
        "        q_list.append(mat_q)\n",
        "\n",
        "   tokens=e.split()\n",
        "   index=tokens.index('<q>')\n",
        "\n",
        "   sub_tokens=[]\n",
        "   for i in range(index,len(tokens)):\n",
        "     if(i>index+4):\n",
        "            break\n",
        "     else:\n",
        "        sub_tokens.append(tokens[i])\n",
        "\n",
        "   if('<q1>' in sub_tokens and ('-' in sub_tokens or 'to' in sub_tokens)):\n",
        "      j=0\n",
        "      for i in range(len(sub_tokens)):\n",
        "\n",
        "       if(sub_tokens[i] in ['-','to']):\n",
        "           req_quant=req_quant+\" \"+sub_tokens[i]\n",
        "\n",
        "       elif(sub_tokens[i] =='<q1>'):\n",
        "          req_quant=req_quant+\" \"+q_list[j]\n",
        "          j=j+1\n",
        "   #end of rangr quantity\n",
        "\n",
        "   #adj in quantity phrase with window length of 2 and select the closeset one if present\n",
        "\n",
        "   mod_quant=org_sent.replace(q,'<q>')\n",
        "   tokens=mod_quant.split()\n",
        "   index=tokens.index('<q>')\n",
        "   sur_tokens=[[],[]]\n",
        "   nps_sent=spacy_noun(mod_quant)\n",
        "   adj=''\n",
        "\n",
        "   for i in range(len(tokens)):\n",
        "         if(abs(i-index)==1):\n",
        "\n",
        "             sur_tokens[0].append(tokens[i])\n",
        "         if(abs(i-index)==2):\n",
        "            sur_tokens[1].append(tokens[i])\n",
        "\n",
        "   for word_list in sur_tokens:\n",
        "     if(word_list):\n",
        "              for j in range(len(word_list)-1,-1,-1):\n",
        "\n",
        "                if(pos_word(word_list[j])=='ADJ'):\n",
        "                  adj_flag=0\n",
        "                  for nps_s in nps_sent:\n",
        "                    if(str(word_list[j]) in str(nps_s)):\n",
        "                          adj_flag=1\n",
        "                  if(adj_flag==0):\n",
        "                        adj=word_list[j]\n",
        "                        break\n",
        "     if(adj):\n",
        "                break\n",
        "   #finding the verbs present in relation\n",
        "   verb=[]\n",
        "   rel_tokens=rel.split()\n",
        "   for w in rel_tokens:\n",
        "       if(pos_word(w)=='VERB'):\n",
        "         verb.append(w)\n",
        "   #verb is other than aux verbs , return the last verb\n",
        "   aux=['has','have','had','is','was','were','are']\n",
        "   verb=[str(w) for w in verb if str(w) not in aux]\n",
        "   if(verb):\n",
        "          verb=str(verb[-1])\n",
        "   else:\n",
        "     verb=''\n",
        "\n",
        "#matching for those quan#tity which has length asscociated and from/ to present with in window length of 2, here we are adding distance word in attribute\n",
        "\n",
        "   if(mat[0].unit.entity.name=='length' and 'from' in sur_tokens[0] and (adj not in adj_noun) ):\n",
        "    #find nouns starting from quantity in sentence\n",
        "              s_n=spacy_noun(start_q)\n",
        "              #spilt the sentence starting from quantity in sentence\n",
        "              start_q_split=start_q.split()\n",
        "              #split the first noun phrase\n",
        "              first_noun_start_q=str(s_n[0]).split()[0]\n",
        "              #find the distance\n",
        "\n",
        "              dis=start_q_split.index(first_noun_start_q)\n",
        "             #if there is noun which is in window length of 4\n",
        "              if(s_n and dis < 4 ):\n",
        "                #if there is no verb just add 'distance from ' and noun\n",
        "                if(not(verb)):\n",
        "\n",
        "                   req_relation='distance from '+str(s_n[0])\n",
        "                #if verb in defined dict lemmatize and add in middle of distance from and noun\n",
        "                elif(verb and lemmatizer.lemmatize(verb) in dist_verbs):\n",
        "                      req_relation='distance from '+str(s_n[0])\n",
        "                #else add just verb\n",
        "                else:\n",
        "                  req_relation='distance '+verb+\" from \"+str(s_n[0])\n",
        "                #get entity from entity_attribute_spl function\n",
        "                req_entity=entity_attribute_spl(ent)\n",
        "              return req_entity,req_relation,req_quant\n",
        "  #instead of 'from', do same for 'to'\n",
        "   elif(mat[0].unit.entity.name=='length'  and 'to' in sur_tokens[0] and (adj not in adj_noun)):\n",
        "              s_n=spacy_noun(start_q)\n",
        "              start_q_split=start_q.split()\n",
        "              first_noun_start_q=str(s_n[0]).split()[0]\n",
        "              dis=start_q_split.index(first_noun_start_q)-start_q.index('<q>')\n",
        "              if(s_n and dis < 4 ):\n",
        "                if(not(verb)):\n",
        "                   req_relation='distance to '+str(s_n[0])\n",
        "                elif(verb and lemmatizer.lemmatize(verb) in dist_verbs):\n",
        "                      req_relation='distance to '+str(s_n[0])\n",
        "                else:\n",
        "                  req_relation='distance '+verb+\" to \"+str(s_n[0])\n",
        "\n",
        "                req_entity=entity_attribute_spl(ent)\n",
        "\n",
        "              return req_entity,req_relation,req_quant\n",
        "   else:\n",
        "\n",
        "\n",
        "  #if adj in quantity phrase is in defined dict, convert adj to noun like old to age, tall to height\n",
        "     if(adj in adj_noun):\n",
        "          adj=adj_noun[adj]\n",
        "    #get entity and attribute from entity_attribute using ent phrase, orginal senetnce\n",
        "     req_entity,attr=entity_attribute(ent,org_sent)\n",
        "     #if we got attribute as empty string, based on words present in window length and quantity unit, we intialised attribute with some strings\n",
        "     if(mat[0].unit.entity.name=='length' and not(attr)):\n",
        "       for sur in sur_tokens:\n",
        "         for sur_in in sur:\n",
        "           if('away' == sur_in):\n",
        "                   attr='distance'\n",
        "\n",
        "     elif(mat[0].unit.entity.name=='length' and not(attr)):\n",
        "       if('feet' in req_quant):\n",
        "         attr='height'\n",
        "       else:\n",
        "         attr='length'\n",
        "     elif(mat[0].unit.entity.name=='currency' and not(attr)):\n",
        "       attr='amount'\n",
        "     elif(mat[0].unit.entity.name=='data storage' and not(attr)):\n",
        "          attr='size'\n",
        "     elif(mat[0].unit.entity.name=='time' and not(attr)):\n",
        "          attr='duration'\n",
        "     #prep_of contains noun phrase related to quantity if we found any in sentence\n",
        "     #modified attr based on certain conditions\n",
        "     if(not adj and prep_of and verb):\n",
        "       if(attr):\n",
        "           attr=attr+\" of \"+only_noun_adj(prep_of)+\" \"+verb\n",
        "       else:\n",
        "         attr= only_noun_adj(prep_of)+\" \"+verb\n",
        "\n",
        "     elif(not adj and prep_of):\n",
        "         if(attr):\n",
        "           attr=attr+\" of \"+only_noun_adj(prep_of)\n",
        "         else:\n",
        "             attr= only_noun_adj(prep_of)\n",
        "\n",
        "     elif(not prep_of and verb):\n",
        "           if(attr):\n",
        "              attr=attr+\" \"+verb+\" \"+adj\n",
        "           else:\n",
        "              attr=mat[0].unit.entity.name+\" \"+verb\n",
        "     elif(not(verb)):\n",
        "       attr=adj+\" \"+attr\n",
        "     return req_entity,attr,req_quant\n",
        "\n",
        " except:\n",
        "    return '','',''\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkQSBznkomNL"
      },
      "outputs": [],
      "source": [
        "#function to extract entity for pattern 1\n",
        "def ner_1(actual_sent,out_text,out_e):\n",
        "  #find all the proper nouns in output sentence\n",
        "    ner_noun_res=ner_noun(out_text)\n",
        "    ner_noun_res=[str(res) for res in ner_noun_res]\n",
        "\n",
        "    doc = nlp(out_e)\n",
        "    matches=parser.parse(actual_sent)\n",
        "    #find all entities in entity phrase\n",
        "    result=[]\n",
        "    for ent in doc.ents:\n",
        "        if (ent.label_!='CARDINAL'):\n",
        "               result.append(ent.text)\n",
        "\n",
        "  #check for entity_and , entity_and_noun patterns, if it exists and only one quantity in sentence , return total entity span\n",
        "\n",
        "    entity_span,entities=entity_and(actual_sent)\n",
        "\n",
        "    if( entity_span and len(matches)==1):\n",
        "\n",
        "        if(any(e in entities for e in result)):\n",
        "\n",
        "                       return entity_span\n",
        "    entity_span,entities=entity_and_noun(actual_sent)\n",
        "    if( entity_span and len(matches)==1):\n",
        "             if(any(e in entities for e in result)):\n",
        "\n",
        "                      return entity_span\n",
        "\n",
        " #only one proper noun, return\n",
        "    if(len(ner_noun_res)==1):\n",
        "\n",
        "             return ner_noun_res[0]\n",
        "\n",
        "  #for more than one , find the nouns in entity and only one return else last one entity as it is close to quantity\n",
        "    if(len(ner_noun_res)>1):\n",
        "\n",
        "          ner_n=ner_noun(out_e)\n",
        "          if(ner_n and\n",
        "             len(ner_n)==1):\n",
        "            return ner_n[0]\n",
        "          f_e=1\n",
        "          print(actual_sent,ner_n[-1])\n",
        "\n",
        "          return ner_n[-1]\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "RYm7UcNxqhLh"
      },
      "outputs": [],
      "source": [
        "#this function takes the noun phrases list and sentence(got by appending entity, relation and modified quantity (replaced quantity by <q>)). Finds the nearest\n",
        "#noun phrase from left and right to quantity (which is here <q>) (by distance to <q>)\n",
        "\n",
        "def nearest_noun(np_list,sent1):\n",
        "    noun_list=[]\n",
        "    np_list=[str(np) for np in np_list]\n",
        "\n",
        "    k=sent1\n",
        "    #Replace each noun phrase in the sentence with a placeholder \"<q0>\", \"<q1>\", \"<q2>\", and so on, and store the modified noun phrases in noun_list.\n",
        "    for i in range(len(np_list)):\n",
        "\n",
        "            w=\"<q\"+str(i)+\">\"\n",
        "\n",
        "            noun_list.append(w)\n",
        "\n",
        "            k=k.replace(np_list[i],w)\n",
        "\n",
        "    words=k.split()\n",
        "    #pos_tags stores the index of the words (key :word , value : index in sentence). note here noun phrases were already replaced by \"<q0>\", \"<q1>\", \"<q2>\", and so on\n",
        "    pos_tags={}\n",
        "    i=0\n",
        "    for word in words:\n",
        "        pos_tags[word]=i\n",
        "        i=i+1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    if(noun_list):\n",
        "            #calculate distance between noun phrases and <q>\n",
        "            distance=abs(pos_tags[noun_list[0]]- pos_tags[\"<q>\"])\n",
        "            result=np_list[0]\n",
        "            for i in range(1,len(noun_list)):\n",
        "                d=abs(pos_tags[noun_list[i]]- pos_tags[\"<q>\"])\n",
        "\n",
        "                if(d < distance):\n",
        "                    distance=d\n",
        "\n",
        "                    result=np_list[i]\n",
        "\n",
        "      #return the nearest noun phrase <q>\n",
        "    return result\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nearest_noun(['length','building'],'The room has <q> of length which is the shortest in this building ')\n",
        "#entity The room\n",
        "#RELATION has\n",
        "#qunatity phrase  <q> of length which is the shortest in this building\n",
        "#since there are no nouns in relation, we check in quantity. due to multiple nouns in quantity , we apply the nearest noun  for ['length','building'] list,\n",
        "#length will be the output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "vyaNUemJx3k5",
        "outputId": "a4f0ffb3-d4d5-4e64-b7d2-5fbeedc5c5e7"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'length'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bH9_hP9FoMSI"
      },
      "outputs": [],
      "source": [
        "#function to extract entity , attribute , quantity for pattern1 matched\n",
        "def output_12(entity,relation,quantity,match,output_sent,sent,verb,unit):\n",
        "\n",
        "    req_entity=ner_1(sent,output_sent,entity) #extract noun phrases which has proper nouns\n",
        "\n",
        "    start,end=match[0].span\n",
        "\n",
        "    req_quantity=quantity[start:end]\n",
        "\n",
        "    sent=sent.replace(req_quantity,'<q>') #replacing quantity in original sentence by <q>\n",
        "    #find whether sentence is matched by any of these three patterns we are getting additional noun phrases related to quantity by 'of'\n",
        "    percentage_match_1=percentage_relation_1(sent)\n",
        "\n",
        "    percentage_match_2=percentage_relation_2(sent)\n",
        "    percentage_match_3=percentage_relation_3(sent)\n",
        "\n",
        "    # replacing quantity in qunatity phrase and output sentence (concatenation of entity , relation,quantity)\n",
        "    modified_q=\"\".join((quantity[:start],\"<q>\",quantity[end:]))\n",
        "    out_sent=\" \".join((entity,relation,modified_q))\n",
        "\n",
        "    req_relation=''\n",
        "   #find what is the verb we got from pattern , if we have '['has','have','had']' it's most likely that only noun phrase itself can be measuring attribute\n",
        "    if verb in ['has','have','had']:\n",
        "        #search for noun phrases in relation\n",
        "        np_list=noun_chunks(relation)\n",
        "\n",
        "       #if no noun phrase found in relation , search for noun phrase in qunatity phrase\n",
        "        if(not np_list):\n",
        "\n",
        "             np_list=noun_chunks(modified_q)\n",
        "\n",
        "        #find the nearest noun phrase to quantity\n",
        "\n",
        "        if(np_list):\n",
        "\n",
        "          req_relation=nearest_noun(np_list,out_sent)\n",
        "\n",
        "        #check and add that noun phrase we got friom pattern ,if any of these pattern matches and that noun phrase we got from pattern is not in already found attribute\n",
        "        if(percentage_match_1):\n",
        "\n",
        "            rel=str(percentage_match_1[0]).split('of <q> ', 1)[-1]\n",
        "            rel_list=rel.split()\n",
        "            req_list=req_relation.split()\n",
        "            if(not(any(i in rel_list for i in req_list))):\n",
        "\n",
        "\n",
        "                         req_relation=str(req_relation)+\" \"+rel\n",
        "        elif(percentage_match_3):\n",
        "\n",
        "            rel=str(percentage_match_3[0]).split('<q> of ', 1)[-1]\n",
        "            rel_list=rel.split()\n",
        "            req_list=req_relation.split()\n",
        "            if(not(any(i in rel_list for i in req_list))):\n",
        "\n",
        "\n",
        "                         req_relation=str(req_relation)+\" \"+rel\n",
        "\n",
        "        elif(percentage_match_2):\n",
        "            rel=str(percentage_match_2[0]).split('<q> ', 1)[-1]\n",
        "            rel_list=rel.split()\n",
        "            req_list=req_relation.split()\n",
        "            if(not(any(i in rel_list for i in req_list))):\n",
        "\n",
        "\n",
        "                         req_relation=str(req_relation)+\" \"+rel\n",
        "    #if verb is not in ['has,had,have'], then we applied the if part same for this, just appened verb with noun phrase we got\n",
        "    else:\n",
        "\n",
        "        np_list=noun_chunks(relation)\n",
        "\n",
        "        if(not np_list):\n",
        "\n",
        "             np_list=noun_chunks(modified_q)\n",
        "\n",
        "\n",
        "        if(np_list):\n",
        "\n",
        "                req_relation=str(nearest_noun(np_list,out_sent))\n",
        "\n",
        "\n",
        "        if(percentage_match_1):\n",
        "\n",
        "            rel=str(percentage_match_1[0]).split('of <q> ', 1)[-1]\n",
        "            rel_list=rel.split()\n",
        "            req_list=req_relation.split()\n",
        "            if(not(any(i in rel_list for i in req_list))):\n",
        "\n",
        "\n",
        "                         req_relation=str(req_relation)+\" \"+rel\n",
        "        elif(percentage_match_3):\n",
        "\n",
        "            rel=str(percentage_match_3[0]).split('<q> of ', 1)[-1]\n",
        "            rel_list=rel.split()\n",
        "            req_list=req_relation.split()\n",
        "\n",
        "            if(not(any(i in rel_list for i in req_list))):\n",
        "\n",
        "\n",
        "                         req_relation=str(req_relation)+\" \"+rel\n",
        "\n",
        "        elif(percentage_match_2):\n",
        "            rel=str(percentage_match_2[0]).split('<q> ', 1)[-1]\n",
        "            rel_list=rel.split()\n",
        "            req_list=req_relation.split()\n",
        "            if(not(any(i in rel_list for i in req_list))):\n",
        "               req_relation=str(req_relation)+\" \"+rel\n",
        "        if(req_relation==''):\n",
        "                 return req_entity,req_relation,req_quantity\n",
        "        if (verb not in req_relation) :\n",
        "                req_relation=verb+\" \"+req_relation\n",
        "\n",
        "\n",
        "    return req_entity,req_relation,req_quantity\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1-qoS82KLmw"
      },
      "outputs": [],
      "source": [
        "c=0\n",
        "p_m,p_m2,output1_no,ouput2_no,both_c=0,0,0,0,0\n",
        "f=0\n",
        "len_values=[]\n",
        "results={}\n",
        "for sent,values in new_d.items():\n",
        "\n",
        "    each_sen_val=[]\n",
        "    bonie=[]\n",
        "\n",
        "    len_values.append(len(values))\n",
        "    flag1,flag2,flag3,flag4,flag5=0,0,0,0,0\n",
        "    #for each output value we got from bonie , we are applying these filterings\n",
        "    for v in values:\n",
        "            try:\n",
        "                   out=re.findall('\\(([^)]+)', v)[-1]\n",
        "                   tuples=out.split(\";\")\n",
        "                   #splitting the string into entity , relation,quantity\n",
        "\n",
        "                   entity,relation,quantity=tuples[0].strip(),tuples[1].strip(),tuples[2].strip()\n",
        "                   match1=parser.parse(quantity) #parsing quantity phrase\n",
        "\n",
        "\n",
        "                  #just removing space in $ qunatity as it was not detected by parser\n",
        "\n",
        "                   quantity=quantity.replace(\"$ \",\"$\")\n",
        "                   modified_sent=sent.replace(\"$ \",\"$\")\n",
        "                   e_s,r_s,q_s=entity,relation,quantity\n",
        "\n",
        "\n",
        "\n",
        "                   length1=len(re.findall(r'\\w+', quantity))\n",
        "                   length2=len(re.findall(r'\\w+', relation))\n",
        "\n",
        "                   # Sometimes variables T and L, which refers to temporal and location those outputs are removed\n",
        "                   #if length is >6 for relation and qunatity phrase those are removed\n",
        "                   if( quantity.startswith('L:') or quantity.startswith('T:') or length1>6 or length2>6):\n",
        "\n",
        "\n",
        "                                pass\n",
        "                   #if there is no quantity or more than one quantity in quantity phrase or quantity in relation phrase, those are removed\n",
        "                   elif ( not(match1) or len(match1) >1 or parser.parse(relation)):\n",
        "\n",
        "                                pass\n",
        "\n",
        "                   else:\n",
        "                       #find the unit\n",
        "                       unit=match1[0].unit.name\n",
        "                       output_sent=\" \".join((entity,relation,quantity)) #concatenate entity,relation,quantity of bonie output\n",
        "                       #check with pattern 1\n",
        "                       verb=pattern_1(output_sent)\n",
        "\n",
        "                       if(verb):\n",
        "                               #pattern matched\n",
        "                                if(flag1==0):\n",
        "                                             flag1=1\n",
        "                                             p_m=p_m+1\n",
        "                                             c=1\n",
        "                                #find the entity , attribute ,relation by output_12 function\n",
        "\n",
        "                                e,r,q=output_12(entity,relation,quantity,match1,output_sent,sent,verb,unit)\n",
        "                                #if we found all three are present in output sentence , take it as one sample for our dataset\n",
        "                                if(r and e and q):\n",
        "\n",
        "                                        each_sen_val.append((str(e),str(r),str(q)))\n",
        "                                        bonie.append((str(e_s),str(r_s),str(q_s)))\n",
        "                                        if(flag2==0):\n",
        "\n",
        "                                             flag2=1\n",
        "                                             output1_no=output1_no+1\n",
        "                      #else check for pattern 2 where atleast one noun in entity and no nouns in quantity and relation phrase\n",
        "                       elif(no_nouns(entity,relation,quantity,match1)):\n",
        "\n",
        "                          if(flag4==0):\n",
        "                                             flag4=1\n",
        "                                             p_m2=p_m2+1\n",
        "                 #if pattern is matched find the entity , attribute ,relation by output_12 function\n",
        "                          e,r,q=ouput2(entity,relation,quantity,match1,modified_sent)\n",
        "                          if(e and r and q):\n",
        "                                each_sen_val.append((str(e),str(r),str(q)))\n",
        "                                bonie.append((str(e_s),str(r_s),str(q_s)))\n",
        "                                if(flag5==0):\n",
        "\n",
        "                                             flag5=1\n",
        "                                             ouput2_no=ouput2_no+1\n",
        "\n",
        "\n",
        "\n",
        "            except:\n",
        "                     pass\n",
        "\n",
        "    if(flag5 and flag2):\n",
        "      #counting how many sentences both pattern matched since for one sentence there may be many outputs\n",
        "\n",
        "                           both_c=both_c+1\n",
        "\n",
        "    results[sent]=(bonie,each_sen_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cz3w9XfPJqgu"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "with open(\"/content/drive/MyDrive/MTP Project/correct_data.json\", 'r') as f:\n",
        "                  sent_data=json.load(f)\n",
        "len(sent_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sent_data is the results output we got above from rule based output\n",
        "#sent_data is dict with keys as sentence and values as [all_bonie_outputs,their_rule_based_output]\n",
        "#all_bonie_outputs,their_rule_based_output is list of list(inner list [entity,relation/attribute,quantity] for each output)"
      ],
      "metadata": {
        "id": "mApGAh1wbLJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data={}\n",
        "for i,v in sent_data.items():\n",
        "  k=[]\n",
        "  bonie,rule=v[0],v[1]\n",
        "  for val in range(len(bonie)):\n",
        "    bonie_output=bonie[val]\n",
        "    rule_output=rule[val]\n",
        "    d={}\n",
        "    d['bonie_entity'],d['bonie_relation'],d['bonie_quantity']=bonie_output[0],bonie_output[1],bonie_output[2]\n",
        "    d['rule_entity'],d['rule_attribute'],d['rule_quantity']=rule_output[0],rule_output[1],rule_output[2]\n",
        "    k.append(d)\n",
        "  data[i]=k\n"
      ],
      "metadata": {
        "id": "-rpp2-1V5hC6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LmxaZtK7yd9",
        "outputId": "7f7bf6cc-7ef1-401c-bcbd-7bb4525c1dc1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "93896"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the combined data to a new JSON file\n",
        "with open(\"/content/drive/MyDrive/MTP Project/rule_dataset.json\", 'w') as f:\n",
        "    json.dump(data, f)"
      ],
      "metadata": {
        "id": "ZcJBE38E70RJ"
      },
      "execution_count": 10,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}